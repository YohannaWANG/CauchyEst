
<img align="left" src="docs/images/Cauchy_est_logo.png"> &nbsp; &nbsp;

   

# Learning Sparse Fixed-Structure Gaussian BayesianNetworks



## Introduction
We study **identifiability** of **Andersson-Madigan-Perlman (AMP)** chain graph models, which are a common generalization of linear structural equation models and Gaussian graphical models. <u>AMP models are described by DAGs on chain components which themselves are undirected graphs.</u> 

For a known chain component decomposition, we show that the DAG on the chain components is identifiable if the determinants of the residual covariance matrices of the chain components are monotone non-decreasing in topological order. This condition extends the equal variance identifiability criterion for Bayes nets, and it can be generalized from determinants to any super-additive function on positive semidefinite matrices. When the component decomposition is  unknown, we describe conditions that allow recovery of the full structure using a polynomial time algorithm based on submodular function minimization. We also conduct experiments comparing our algorithm's performance against existing baselines.                                       

                                                            
## Prerequisites
- Python 3.6+
- `networkx`
- `argpase`
- `itertools`
- `numpy`
- `scipy`
- `sklearn`
- `matplotlib`
- `torch`: Optional, only used for nonlinear model.

- R 4.0.0
- `rpy2`: Python interface, enables calling R from Python. Install [rpy2](https://pypi.org/project/rpy2/) first.
- `bnlearn` : [Bayesian network learning and inference](bnlearn.com) 
- `glasso` : [Graphical Lasso: Estimation of Gaussian Graphical Models](https://cran.r-project.org/web/packages/glasso/index.html)
- `flare`: [Family of Lasso Regression](https://cran.r-project.org/web/packages/flare/index.html)

## Contents
- Data  - Real Bayesian network data from bnlearn;
- `data.py` - generate synthetic chain graph data, including graph simulation and data simulation. Load real Bnlearn data 
- `evaluate.py` - algorithm accuracy evaluation 
- `config.py` - Set parameters for Bayesian network (eg. node number, graph degree, sample size)
- `utils.py` - simulation parameters, such as selecte graph type, node number, data type, graph degree, etc.  
- `utils.R` - load bnlearn graph; Run CLIME algorithm


## Running a simple demo


## Runing as a command

## Algorithms
<img width="460" alt="characterization" src="/docs/images/algo1.png"/>    

## Performance

## Citation

## Contacts

Please feel free to contact us if you meet any problem when using this code. We are glad to hear other advise and update our work. 
We are also open to collaboration if you think that you are working on a problem that we might be interested in it.
Please do not hestitate to contact us!







## TODO tasks (eval.py, algo.py, and main.py)

```diff
+ (Done) General synthetic SEM data;
+ (Done) (FIXED) <**R language**>: bnlearn R data
+ (Done) Tree structure synthetic data;
 
+ (Done) DAG: Liear regression algorithm;
+ (Done) DAG: Least square algorithm;
+ (Done) Undirected graph: GLASSO algotirhm;
+ (Done) Undirected graph: empirical estimator;
 
+ (Done) Performance evaluation (KL-distance) on DAG;
+ (Done) Performance evaluation (KL-distance) on Undirected graph;

+ (Done) Overleaf: algorithm 1;
+ (Done) Overleaf: algorithm 2;
+ (Done) Overleaf: algorithm 3;
+ (Done) Overleaf: algorithm 4;
+ (Done) Add R CLIME & TIGER algorithm;
+ (Done) Code for generate plot;
+ (Done) Data: synthetic ill-conditioned data;
+ (Done) Experiments: Ill-conditioned models. Like say one of the variables has noise variance very close to 0;
+ (Done) Experiments: how the error decreases for empirical and GLASSO on separate plots;


- (Done) For distributions generated by degree 10 Bayes network, run our three algorithms with d=5. See how fast the errors converge.
- (Done) Real datasets from bnlearn
- (Done) Ill-conditioned models. Like say one of the variables has noise variance very close to 0. Hopefully, here we can also find some difference between CauchyEst and CauchyEstGeneral;
- (Done) some fraction have  N = r.normal(scale = 10^-10) others have N = r.normal(scale = 1.0) as before;
- (Done) for each server job, try running on "1 algorithm, 1 parameter setting, X samples". that way you can parallelize quite a lot and you can plot each graph line (with error bars) as soon as one of these jobs complete.
```




